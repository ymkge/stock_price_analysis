{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 必要なライブラリをインストール\n",
    "!pip install pandas requests tqdm lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. メインのPythonコード\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "def create_debug_file():\n",
    "    \"\"\"\n",
    "    【デバッグ専用】スクレイピングした生のデータをCSVファイルに出力する。\n",
    "    \"\"\"\n",
    "    print(\"Yahoo!ファイナンスの時価総額ランキングからデバッグ用の生データを取得します...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    base_url = \"https://finance.yahoo.co.jp/stocks/ranking/marketCapitalHigh?market=all&page={}\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    max_page = 5 # デバッグなので、ページ数を5に減らして高速化します\n",
    "    for page in tqdm(range(1, max_page + 1), desc=\"ランキングページ取得中\"):\n",
    "        try:\n",
    "            url = base_url.format(page)\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            tables = pd.read_html(io.StringIO(response.text))\n",
    "            \n",
    "            if len(tables) > 0:\n",
    "                all_dfs.append(tables[0])\n",
    "            else:\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"ページ {page} の取得中にエラーが発生しました: {e}\")\n",
    "            break\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"データを取得できませんでした。\")\n",
    "        return\n",
    "\n",
    "    # --- 取得した全データを結合 ---\n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # ★★★ ここが今回の目的 ★★★\n",
    "    # 取得した生のDataFrameを、一切加工せずにCSVファイルとして保存する\n",
    "    csv_filename = 'raw_scraping_data.csv'\n",
    "    try:\n",
    "        # encoding='utf-8-sig' はExcelで文字化けしないためのおまじない\n",
    "        full_df.to_csv(csv_filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n--- 【デバッグ用ファイル出力完了】 ---\")\n",
    "        print(f\"スクレイピングした生データを '{csv_filename}' に保存しました。\")\n",
    "        print(\"\\n★★★ 次のステップ ★★★\")\n",
    "        print(\"1. Google Colabの左側にあるフォルダアイコンをクリックしてファイル一覧を表示してください。\")\n",
    "        print(f\"2. '{csv_filename}' というファイルが表示されているはずです。\")\n",
    "        print(\"3. このファイルをダウンロードし、Excelやテキストエディタで開いてください。\")\n",
    "        print(\"4. 特に `'名称・コード・市場'` 列のセルの中身が、**具体的にどのような文字列になっているか**を確認し、その内容を教えてください。\")\n",
    "        print(\"（例：「トヨタ自動車(株) 7203 東証PRM 掲示板」のようにスペース区切りか、「トヨタ自動車(株)7203東証PRM掲示板」のように連結されているか、など）\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCSVファイルへの保存中にエラーが発生しました: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n処理が完了しました。所要時間: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_debug_file()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
